<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/README.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/README.md" />
              <option name="originalContent" value="# Vision-Transformers-for-Histopathology" />
              <option name="updatedContent" value="# CPU-Only Vision Transformers for Histopathology: Distillation, EMA, and INT8 Quantization with Calibration-Aware Evaluation&#10;&#10;Код экспериментального пайплайна для CPU-деплоя ViT: KD → EMA → INT8 (PTQ) и короткий QAT-lite. Поддержаны сплиты train/val/test MedMNIST и множественные прогоны по фиксированным seeds. Выход — CSV с качеством и CPU-метриками.&#10;&#10;## Цели&#10;- Показать CPU-деплой ViT (DeiT-T) через связку KD → EMA → INT8 (PTQ) и понять, когда PTQ достаточно, а когда нужна короткая QAT-lite.&#10;- Дать формулы методов (KD, EMA), описать PTQ/QAT-lite.&#10;- Улучшить экспериментальную оценку: валидируем настройки по val, тестируем один раз без аугментаций, считаем среднее ± ст. откл. по seeds.&#10;&#10;## Установка&#10;```&#10;pip install -r requirements.txt&#10;```&#10;&#10;## Запуск экспериментов&#10;Скрипт: `scripts/run_experiments.py`&#10;- Данные: PathMNIST (MedMNIST) с официальными train/val/test.&#10;- Модели: Teacher DeiT-S @224, Student DeiT-T (FP32 и INT8), бейзлайны ResNet-18, MobileNetV3-S, ConvNeXt-T.&#10;- Квантование: ViT — динамическое PTQ (Linear), CNN — статическое PTQ (FX) при наличии, иначе динамическое. QAT-lite (FX) где поддерживается.&#10;&#10;Пример запуска:&#10;```&#10;python scripts/run_experiments.py --dataset pathmnist \&#10;  --epochs_teacher 1 --epochs_student 1 --qat_epochs 3 \&#10;  --batch_size 64 --seeds 42 43 44 45 46&#10;```&#10;Результаты сохраняются в `results/results.csv`.&#10;&#10;Колонки CSV (основные):&#10;- Качество: accuracy, macro_f1, macro_precision, macro_recall, roc_auc_macro&#10;- CPU: latency p50/p90 (batch=1,8), throughput (img/s), peak RAM (MB), size (MB)&#10;- Модель: params (M), GFLOPs@224, роль/метод/точность&#10;- CPU/ОС: базовые поля CPU из системы&#10;&#10;Повторите запуск на втором CPU (например, ноутбук и десктоп/встроенный), объедините CSV для сравнения.&#10;&#10;## Протокол оценки&#10;- Разделение: официальные train/val/test MedMNIST. Все гиперпараметры и выбор чекпойнта — по val. Test — один прогон на `model.eval()` без аугментаций.&#10;- Многократные прогоны: по умолчанию 5 фиксированных `--seeds`.&#10;- Альтернатива: 5× стратифицированные повторы доступны утилитой `utils/data.py:stratified_kfold_splits` (не включено в скрипт по умолчанию).&#10;&#10;## Методы и формулы&#10;- EMA весов: θ_ema ← decay·θ_ema + (1−decay)·θ (обновляются только float-параметры/буферы). Реализация: `EMA_for_weights.py`.&#10;- KD (Hinton): L = (1−α)·CE(y_s, y) + α·T²·KL(softmax(z_t/T) || softmax(z_s/T)). Реализация: `utils/kd.py`.&#10;- PTQ:&#10;  - ViT: динамическая INT8 для Linear (веса int8, активации динамически). `utils/quant.py:dynamic_int8`.&#10;  - CNN: статическая INT8 (FX) с калибровкой `calib_batches` на train. `utils/quant.py:static_ptq_fx`.&#10;- QAT-lite: 3–10 эпох fake-quant (FX) с малым LR, опционально с KD. `utils/quant.py:qat_lite_fx`.&#10;&#10;Опишите в тексте объём калибровки (число батчей), падение качества vs FP32, и когда QAT-lite улучшает PTQ.&#10;&#10;## CPU-бенчмарки&#10;- Бэкенд: PyTorch (eager). Для расширения можно добавить TorchScript/ONNXRuntime.&#10;- Метрики: latency p50/p90 (мс) при batch=1 (доп. batch=8), throughput (img/s), peak RAM (MB), size (MB). Прогрев 50 итераций, ≥100 замеров, 3 повтора. Реализация: `utils/bench.py`.&#10;- Записывайте CPU/ОС/драйверы в текст работы. Опционально измерять энергию (RAPL) — не реализовано в коде.&#10;&#10;## Метрики качества (медицинский фокус)&#10;`utils/metrics.py` считает:&#10;- Accuracy, Macro-F1, Macro-Precision/Recall&#10;- ROC-AUC (macro, OvR) при наличии всех классов&#10;- Confusion matrix и per-class precision/recall/f1 (внутренние поля)&#10;&#10;## Абляции&#10;В `scripts/run_experiments.py` включены варианты:&#10;- KD, EMA, KD+EMA (FP32), затем +PTQ (INT8)&#10;- QAT-lite (INT8) при поддержке FX&#10;Для каждого варианта пишутся качество и CPU-метрики.&#10;&#10;## Параметры/FLOPs/Размер&#10;- Params (М): по числу параметров&#10;- GFLOPs@224: через `ptflops` (если доступен)&#10;- Size (MB): по state_dict&#10;&#10;## Ограничения&#10;- FX-пайплайн PTQ/QAT-lite может не поддерживаться некоторыми ViT. Для DeiT-T мы используем динамическое PTQ.&#10;- Энергия (RAPL) и бэкенды TorchScript/ONNX не включены по умолчанию.&#10;&#10;## Как воспроизвести&#10;1) Установить зависимости, запустить скрипт с фиксированными seeds на одном CPU.&#10;2) Повторить на втором CPU.&#10;3) Свести CSV в таблицы: среднее ± ст. откл. по seeds и по машинам.&#10;&#10;## Структура статьи (шаблон)&#10;- Аннотация: проблема → метод → данные → качество+CPU → калибровка/робастность → вывод&#10;- Введение: мотив/разрыв/вклад&#10;- Методы: KD/EMA/PTQ/QAT-lite, что квантовано (слои/опции), формулы&#10;- Эксперименты: данные, сплиты, seeds, железо, метрики, статистика&#10;- Результаты: таблицы (среднее ± ст. откл.), абляции&#10;- Обсуждение: когда PTQ достаточно, когда нужна QAT-lite&#10;- Ограничения&#10;- Вывод&#10;- Воспроизводимость/Этика: ссылки на код/артефакты&#10;&#10;## Литература (минимальный набросок)&#10;- Knowledge Distillation: Hinton et al., 2015&#10;- DeiT: Touvron et al., 2021&#10;- Quantization Aware Training/PTQ: Jacob et al., 2018; Banner et al., 2018; Nagel et al., 2020 (AdaRound); Bhandare et al., 2019&#10;- Vision Transformers: Dosovitskiy et al., 2020; Touvron et al., 2022 (DeiT v2)&#10;- Efficient CNN baselines: He et al., 2016 (ResNet); Howard et al., 2019 (MobileNetV3); Liu et al., 2022 (ConvNeXt)&#10;- Медицинские изображения/гистопатология: соответствующие обзоры/датасеты MedMNIST&#10;&#10;Заполните ~20 релевантных источников с корректными цитатами." />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/requirements.txt">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/requirements.txt" />
              <option name="updatedContent" value="torch&gt;=2.1&#10;torchvision&gt;=0.16&#10;timm&gt;=0.9.12&#10;medmnist&gt;=3.0.1&#10;numpy&gt;=1.24&#10;pandas&gt;=2.0&#10;scikit-learn&gt;=1.3&#10;matplotlib&gt;=3.7&#10;seaborn&gt;=0.12&#10;tqdm&gt;=4.66&#10;ptflops&gt;=0.7&#10;psutil&gt;=5.9&#10;onnx&gt;=1.14&#10;onnxruntime&gt;=1.18&#10;onnxruntime-tools&gt;=1.7&#10;py-cpuinfo&gt;=9.0&#10;pyyaml&gt;=6.0&#10;scipy&gt;=1.11&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/scripts/run_experiments.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/scripts/run_experiments.py" />
              <option name="updatedContent" value="import os&#10;import csv&#10;import argparse&#10;from copy import deepcopy&#10;from typing import Dict, Any, Tuple&#10;&#10;import torch&#10;import torch.nn as nn&#10;import torch.optim as optim&#10;import timm&#10;&#10;from utils.seed import set_seed&#10;from utils.data import get_medmnist, build_loaders&#10;from utils.kd import DistillationLoss&#10;from utils.metrics import evaluate&#10;from utils.bench import benchmark_torch, measure_model_size_mb, cpu_info&#10;from utils.quant import dynamic_int8, static_ptq_fx, qat_lite_fx&#10;from EMA_for_weights import EMA&#10;&#10;try:&#10;    from ptflops import get_model_complexity_info&#10;except Exception:&#10;    get_model_complexity_info = None&#10;&#10;&#10;def count_params(model: nn.Module) -&gt; float:&#10;    return sum(p.numel() for p in model.parameters()) / 1e6  # M&#10;&#10;&#10;def compute_flops(model: nn.Module, input_res=(3, 224, 224)) -&gt; float:&#10;    if get_model_complexity_info is None:&#10;        return float('nan')&#10;    try:&#10;        macs, params = get_model_complexity_info(model, input_res, as_strings=False, print_per_layer_stat=False)&#10;        flops = macs * 2  # MACs -&gt; FLOPs approximation&#10;        return float(flops / 1e9)  # GFLOPs&#10;    except Exception:&#10;        return float('nan')&#10;&#10;&#10;def build_model(name: str, num_classes: int) -&gt; nn.Module:&#10;    return timm.create_model(name, pretrained=True, num_classes=num_classes)&#10;&#10;&#10;def train_one_epoch(model, loader, optimizer, device, teacher=None, kd: DistillationLoss = None, ema: EMA = None):&#10;    model.train()&#10;    for x, y in loader:&#10;        x, y = x.to(device), y.to(device)&#10;        if y.ndim &gt; 1:&#10;            y = y.argmax(dim=1)&#10;        optimizer.zero_grad()&#10;        z_s = model(x)&#10;        if teacher is not None and kd is not None:&#10;            with torch.no_grad():&#10;                z_t = teacher(x)&#10;            loss = kd(z_s, z_t, y)&#10;        else:&#10;            loss = nn.functional.cross_entropy(z_s, y.long())&#10;        loss.backward()&#10;        optimizer.step()&#10;        if ema is not None:&#10;            ema.update(model)&#10;&#10;&#10;def fit(model, train_loader, val_loader, device, epochs=1, lr=5e-5, teacher=None, use_kd=False, use_ema=False,&#10;        kd_alpha=0.7, kd_temp=4.0) -&gt; Tuple[nn.Module, Dict[str, float]]:&#10;    optimizer = optim.AdamW(model.parameters(), lr=lr)&#10;    ema = EMA(model, decay=0.999) if use_ema else None&#10;    kd_loss = DistillationLoss(alpha=kd_alpha, temperature=kd_temp) if use_kd else None&#10;&#10;    best_model = deepcopy(model).cpu()&#10;    best_acc = -1.0&#10;&#10;    for _ in range(epochs):&#10;        train_one_epoch(model, train_loader, optimizer, device, teacher=teacher, kd=kd_loss, ema=ema)&#10;        # оценка по val&#10;        metrics_val = evaluate(ema.ema_model if use_ema else model, val_loader, device)&#10;        if metrics_val['accuracy'] &gt; best_acc:&#10;            best_acc = metrics_val['accuracy']&#10;            best_model = deepcopy(ema.ema_model if use_ema else model).cpu()&#10;&#10;    # финальная валидация&#10;    return best_model, {'val_accuracy': best_acc}&#10;&#10;&#10;def evaluate_and_benchmark(model, test_loader, device_cpu, batch_sizes=(1, 8)) -&gt; Dict[str, Any]:&#10;    quality = evaluate(model.to(device_cpu), test_loader, device_cpu)&#10;    bench = benchmark_torch(model, input_shape=(1, 3, 224, 224), device=device_cpu, batch_sizes=batch_sizes)&#10;    return {'quality': quality, 'bench': bench}&#10;&#10;&#10;def add_model_stats(row: Dict[str, Any], model: nn.Module, state_path: str = None):&#10;    row['params_M'] = round(count_params(model), 3)&#10;    row['size_MB'] = round(measure_model_size_mb(state_dict_path=state_path, model=model), 2)&#10;    row['gflops_224'] = round(compute_flops(deepcopy(model).cpu()), 2)&#10;&#10;&#10;def main():&#10;    parser = argparse.ArgumentParser()&#10;    parser.add_argument('--dataset', type=str, default='pathmnist')&#10;    parser.add_argument('--epochs_teacher', type=int, default=1)&#10;    parser.add_argument('--epochs_student', type=int, default=1)&#10;    parser.add_argument('--qat_epochs', type=int, default=3)&#10;    parser.add_argument('--batch_size', type=int, default=64)&#10;    parser.add_argument('--seeds', type=int, nargs='+', default=[42, 43, 44, 45, 46])&#10;    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')&#10;    parser.add_argument('--out', type=str, default='results/results.csv')&#10;    args = parser.parse_args()&#10;&#10;    os.makedirs(os.path.dirname(args.out), exist_ok=True)&#10;&#10;    device_train = torch.device(args.device)&#10;    device_cpu = torch.device('cpu')&#10;&#10;    # данные&#10;    train_ds, val_ds, test_ds, num_classes = get_medmnist(args.dataset)&#10;    train_loader, val_loader, test_loader = build_loaders(train_ds, val_ds, test_ds, batch_size=args.batch_size)&#10;&#10;    # модели&#10;    configs = [&#10;        (&quot;deit_small_patch16_224&quot;, &quot;Teacher&quot;),&#10;        (&quot;deit_tiny_patch16_224&quot;, &quot;Student&quot;),&#10;        (&quot;resnet18&quot;, &quot;Baseline&quot;),&#10;        (&quot;mobilenetv3_small_100&quot;, &quot;Baseline&quot;),&#10;        (&quot;convnext_tiny&quot;, &quot;Baseline&quot;),&#10;    ]&#10;&#10;    # подготовка CSV&#10;    header = [&#10;        'seed','model','role','precision','method','val_acc',&#10;        'acc','macro_f1','macro_precision','macro_recall','roc_auc_macro',&#10;        'lat_p50_b1_ms','lat_p90_b1_ms','thr_b1','lat_p50_b8_ms','lat_p90_b8_ms','thr_b8',&#10;        'params_M','gflops_224','size_MB','cpu_brand','cpu_arch','cpu_bits','cpu_count'&#10;    ]&#10;    cpu_meta = cpu_info()&#10;&#10;    with open(args.out, 'w', newline='') as f:&#10;        writer = csv.DictWriter(f, fieldnames=header)&#10;        writer.writeheader()&#10;&#10;        for seed in args.seeds:&#10;            set_seed(seed)&#10;            # 1) Teacher&#10;            teacher = build_model('deit_small_patch16_224', num_classes).to(device_train)&#10;            teacher_best, m = fit(teacher, train_loader, val_loader, device_train, epochs=args.epochs_teacher, lr=1e-4, use_kd=False, use_ema=True)&#10;            # сохранить веса учителя&#10;            t_path = f&quot;results/teacher_seed{seed}.pt&quot;&#10;            os.makedirs('results', exist_ok=True)&#10;            torch.save(teacher_best.state_dict(), t_path)&#10;&#10;            # 2) Student FP32 with KD and EMA&#10;            student = build_model('deit_tiny_patch16_224', num_classes).to(device_train)&#10;            teacher_for_kd = deepcopy(teacher_best).to(device_train).eval()&#10;            student_best, m = fit(student, train_loader, val_loader, device_train, epochs=args.epochs_student, lr=5e-5, teacher=teacher_for_kd, use_kd=True, use_ema=True)&#10;&#10;            # абляции&#10;            ablations = [&#10;                (&quot;FP32&quot;, &quot;KD&quot;, True, False, False),&#10;                (&quot;FP32&quot;, &quot;EMA&quot;, False, True, False),&#10;                (&quot;FP32&quot;, &quot;KD+EMA&quot;, True, True, False),&#10;                (&quot;INT8&quot;, &quot;PTQ&quot;, False, False, True),&#10;                (&quot;INT8&quot;, &quot;KD+PTQ&quot;, True, False, True),&#10;                (&quot;INT8&quot;, &quot;EMA+PTQ&quot;, False, True, True),&#10;                (&quot;INT8&quot;, &quot;KD+EMA+PTQ&quot;, True, True, True),&#10;            ]&#10;&#10;            # подготовить альтернативные студенты для честной абляции&#10;            base_student = build_model('deit_tiny_patch16_224', num_classes).to(device_train)&#10;            # без KD/EMA&#10;            stud_plain, _ = fit(deepcopy(base_student), train_loader, val_loader, device_train, epochs=args.epochs_student, lr=5e-5, use_kd=False, use_ema=False)&#10;            # KD&#10;            stud_kd, _ = fit(deepcopy(base_student), train_loader, val_loader, device_train, epochs=args.epochs_student, lr=5e-5, teacher=teacher_for_kd, use_kd=True, use_ema=False)&#10;            # EMA&#10;            stud_ema, _ = fit(deepcopy(base_student), train_loader, val_loader, device_train, epochs=args.epochs_student, lr=5e-5, use_kd=False, use_ema=True)&#10;            # KD+EMA (это и есть student_best)&#10;            stud_kd_ema = deepcopy(student_best)&#10;&#10;            fp32_variants = {&#10;                'KD': stud_kd,&#10;                'EMA': stud_ema,&#10;                'KD+EMA': stud_kd_ema,&#10;            }&#10;&#10;            # QAT-lite на студенте (коротко)&#10;            try:&#10;                qatl = qat_lite_fx(deepcopy(stud_kd_ema), train_loader, val_loader=None, epochs=args.qat_epochs, lr=1e-5, device=device_cpu, teacher=None)&#10;                qatl_variant = (&quot;INT8&quot;, &quot;QAT-lite&quot;, qatl)&#10;            except Exception:&#10;                qatl_variant = None&#10;&#10;            # Бейзлайны (FP32 + PTQ/QAT-lite где разумно)&#10;            baselines = [&#10;                ('resnet18', 'ResNet-18'),&#10;                ('mobilenetv3_small_100', 'MobileNetV3-S'),&#10;                ('convnext_tiny', 'ConvNeXt-T'),&#10;            ]&#10;&#10;            # Запись учителя (FP32)&#10;            qual = evaluate_and_benchmark(teacher_best, test_loader, device_cpu)&#10;            row = {&#10;                'seed': seed, 'model': 'DeiT-S', 'role': 'Teacher', 'precision': 'FP32', 'method': 'EMA',&#10;                'val_acc': m['val_accuracy'],&#10;                'acc': qual['quality']['accuracy'], 'macro_f1': qual['quality']['macro_f1'],&#10;                'macro_precision': qual['quality']['macro_precision'], 'macro_recall': qual['quality']['macro_recall'],&#10;                'roc_auc_macro': qual['quality']['roc_auc_macro'],&#10;                'lat_p50_b1_ms': qual['bench'][1]['latency_p50_ms'], 'lat_p90_b1_ms': qual['bench'][1]['latency_p90_ms'], 'thr_b1': qual['bench'][1]['throughput_img_s'],&#10;                'lat_p50_b8_ms': qual['bench'][8]['latency_p50_ms'], 'lat_p90_b8_ms': qual['bench'][8]['latency_p90_ms'], 'thr_b8': qual['bench'][8]['throughput_img_s'],&#10;                'cpu_brand': cpu_meta.get('brand_raw',''), 'cpu_arch': cpu_meta.get('arch',''), 'cpu_bits': cpu_meta.get('bits',''), 'cpu_count': cpu_meta.get('count','')&#10;            }&#10;            add_model_stats(row, teacher_best, t_path)&#10;            writer.writerow(row)&#10;&#10;            # Абляции студента&#10;            for prec, name, use_kd_flag, use_ema_flag, do_ptq in ablations:&#10;                model_fp32 = fp32_variants.get(name)&#10;                if model_fp32 is None:&#10;                    continue&#10;                model_to_eval = deepcopy(model_fp32)&#10;                method_name = name&#10;                if do_ptq:&#10;                    model_to_eval.eval()&#10;                    model_to_eval = dynamic_int8(model_to_eval, modules={nn.Linear})&#10;                    method_name = name + '+PTQ' if 'PTQ' not in name else name&#10;                qual = evaluate_and_benchmark(model_to_eval, test_loader, device_cpu)&#10;                row = {&#10;                    'seed': seed, 'model': 'DeiT-T', 'role': 'Student', 'precision': prec, 'method': method_name,&#10;                    'val_acc': float('nan'),&#10;                    'acc': qual['quality']['accuracy'], 'macro_f1': qual['quality']['macro_f1'],&#10;                    'macro_precision': qual['quality']['macro_precision'], 'macro_recall': qual['quality']['macro_recall'],&#10;                    'roc_auc_macro': qual['quality']['roc_auc_macro'],&#10;                    'lat_p50_b1_ms': qual['bench'][1]['latency_p50_ms'], 'lat_p90_b1_ms': qual['bench'][1]['latency_p90_ms'], 'thr_b1': qual['bench'][1]['throughput_img_s'],&#10;                    'lat_p50_b8_ms': qual['bench'][8]['latency_p50_ms'], 'lat_p90_b8_ms': qual['bench'][8]['latency_p90_ms'], 'thr_b8': qual['bench'][8]['throughput_img_s'],&#10;                    'cpu_brand': cpu_meta.get('brand_raw',''), 'cpu_arch': cpu_meta.get('arch',''), 'cpu_bits': cpu_meta.get('bits',''), 'cpu_count': cpu_meta.get('count','')&#10;                }&#10;                add_model_stats(row, model_to_eval, None)&#10;                writer.writerow(row)&#10;&#10;            # QAT-lite запись&#10;            if qatl_variant is not None:&#10;                _, method_name, qmodel = qatl_variant&#10;                qual = evaluate_and_benchmark(qmodel, test_loader, device_cpu)&#10;                row = {&#10;                    'seed': seed, 'model': 'DeiT-T', 'role': 'Student', 'precision': 'INT8', 'method': method_name,&#10;                    'val_acc': float('nan'),&#10;                    'acc': qual['quality']['accuracy'], 'macro_f1': qual['quality']['macro_f1'],&#10;                    'macro_precision': qual['quality']['macro_precision'], 'macro_recall': qual['quality']['macro_recall'],&#10;                    'roc_auc_macro': qual['quality']['roc_auc_macro'],&#10;                    'lat_p50_b1_ms': qual['bench'][1]['latency_p50_ms'], 'lat_p90_b1_ms': qual['bench'][1]['latency_p90_ms'], 'thr_b1': qual['bench'][1]['throughput_img_s'],&#10;                    'lat_p50_b8_ms': qual['bench'][8]['latency_p50_ms'], 'lat_p90_b8_ms': qual['bench'][8]['latency_p90_ms'], 'thr_b8': qual['bench'][8]['throughput_img_s'],&#10;                    'cpu_brand': cpu_meta.get('brand_raw',''), 'cpu_arch': cpu_meta.get('arch',''), 'cpu_bits': cpu_meta.get('bits',''), 'cpu_count': cpu_meta.get('count','')&#10;                }&#10;                add_model_stats(row, qmodel, None)&#10;                writer.writerow(row)&#10;&#10;            # Бейзлайны&#10;            for mname, label in baselines:&#10;                base = build_model(mname, num_classes).to(device_train)&#10;                base_best, _ = fit(base, train_loader, val_loader, device_train, epochs=args.epochs_student, lr=1e-4, use_kd=False, use_ema=True)&#10;                # FP32&#10;                qual = evaluate_and_benchmark(base_best, test_loader, device_cpu)&#10;                row = {&#10;                    'seed': seed, 'model': label, 'role': 'Baseline', 'precision': 'FP32', 'method': 'EMA',&#10;                    'val_acc': float('nan'),&#10;                    'acc': qual['quality']['accuracy'], 'macro_f1': qual['quality']['macro_f1'],&#10;                    'macro_precision': qual['quality']['macro_precision'], 'macro_recall': qual['quality']['macro_recall'],&#10;                    'roc_auc_macro': qual['quality']['roc_auc_macro'],&#10;                    'lat_p50_b1_ms': qual['bench'][1]['latency_p50_ms'], 'lat_p90_b1_ms': qual['bench'][1]['latency_p90_ms'], 'thr_b1': qual['bench'][1]['throughput_img_s'],&#10;                    'lat_p50_b8_ms': qual['bench'][8]['latency_p50_ms'], 'lat_p90_b8_ms': qual['bench'][8]['latency_p90_ms'], 'thr_b8': qual['bench'][8]['throughput_img_s'],&#10;                    'cpu_brand': cpu_meta.get('brand_raw',''), 'cpu_arch': cpu_meta.get('arch',''), 'cpu_bits': cpu_meta.get('bits',''), 'cpu_count': cpu_meta.get('count','')&#10;                }&#10;                add_model_stats(row, base_best, None)&#10;                writer.writerow(row)&#10;                # INT8 PTQ (static for CNN if possible; otherwise dynamic)&#10;                try:&#10;                    qbase = static_ptq_fx(deepcopy(base_best), calib_loader=train_loader, backend='fbgemm', calib_batches=50)&#10;                    precision = 'INT8'&#10;                    method = 'PTQ-static'&#10;                except Exception:&#10;                    qbase = dynamic_int8(deepcopy(base_best), modules={nn.Linear, nn.Conv2d})&#10;                    precision = 'INT8'&#10;                    method = 'PTQ-dynamic'&#10;                qual = evaluate_and_benchmark(qbase, test_loader, device_cpu)&#10;                row = {&#10;                    'seed': seed, 'model': label, 'role': 'Baseline', 'precision': precision, 'method': method,&#10;                    'val_acc': float('nan'),&#10;                    'acc': qual['quality']['accuracy'], 'macro_f1': qual['quality']['macro_f1'],&#10;                    'macro_precision': qual['quality']['macro_precision'], 'macro_recall': qual['quality']['macro_recall'],&#10;                    'roc_auc_macro': qual['quality']['roc_auc_macro'],&#10;                    'lat_p50_b1_ms': qual['bench'][1]['latency_p50_ms'], 'lat_p90_b1_ms': qual['bench'][1]['latency_p90_ms'], 'thr_b1': qual['bench'][1]['throughput_img_s'],&#10;                    'lat_p50_b8_ms': qual['bench'][8]['latency_p50_ms'], 'lat_p90_b8_ms': qual['bench'][8]['latency_p90_ms'], 'thr_b8': qual['bench'][8]['throughput_img_s'],&#10;                    'cpu_brand': cpu_meta.get('brand_raw',''), 'cpu_arch': cpu_meta.get('arch',''), 'cpu_bits': cpu_meta.get('bits',''), 'cpu_count': cpu_meta.get('count','')&#10;                }&#10;                add_model_stats(row, qbase, None)&#10;                writer.writerow(row)&#10;&#10;    print(f&quot;Saved: {args.out}&quot;)&#10;&#10;&#10;if __name__ == '__main__':&#10;    main()&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/utils/bench.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/utils/bench.py" />
              <option name="originalContent" value="import time&#10;import statistics as stats&#10;from typing import Dict, List, Tuple, Optional&#10;&#10;import numpy as np&#10;import torch&#10;import psutil&#10;import os&#10;&#10;&#10;def _percentiles(xs: List[float], ps=(50, 90, 95, 99)) -&gt; Dict[str, float]:&#10;    arr = np.array(xs)&#10;    out = {}&#10;    for p in ps:&#10;        out[f&quot;p{p}&quot;] = float(np.percentile(arr, p))&#10;    return out&#10;&#10;&#10;def measure_model_size_mb(state_dict_path: Optional[str] = None, model: Optional[torch.nn.Module] = None) -&gt; float:&#10;    if state_dict_path and os.path.exists(state_dict_path):&#10;        return os.path.getsize(state_dict_path) / (1024 * 1024)&#10;    if model is not None:&#10;        tmp = &quot;_tmp_model_size.pt&quot;&#10;        torch.save(model.state_dict(), tmp)&#10;        size = os.path.getsize(tmp) / (1024 * 1024)&#10;        os.remove(tmp)&#10;        return size&#10;    return float(&quot;nan&quot;)&#10;&#10;&#10;def benchmark_torch(&#10;    model: torch.nn.Module,&#10;    input_shape: Tuple[int, int, int, int] = (1, 3, 224, 224),&#10;    device: torch.device = torch.device(&quot;cpu&quot;),&#10;    batch_sizes: Tuple[int, ...] = (1, 8),&#10;    warmup: int = 50,&#10;    iters: int = 100,&#10;    repeats: int = 3,&#10;    num_threads: Optional[int] = None,&#10;) -&gt; Dict[int, Dict[str, float]]:&#10;    &quot;&quot;&quot;&#10;    Return metrics per batch size:&#10;      - latency_p50/p90 (ms)&#10;      - throughput_img_s&#10;      - peak_ram_mb (RSS delta)&#10;    &quot;&quot;&quot;&#10;    if num_threads is not None:&#10;        torch.set_num_threads(num_threads)&#10;&#10;    model.eval().to(device)&#10;&#10;    results = {}&#10;    proc = psutil.Process(os.getpid())&#10;&#10;    for bs in batch_sizes:&#10;        x = torch.randn(bs, *input_shape[1:], device=device)&#10;        # warmup&#10;        with torch.no_grad():&#10;            for _ in range(warmup):&#10;                _ = model(x)&#10;        latencies = []&#10;        peak_rss = 0&#10;        for _ in range(repeats):&#10;            with torch.no_grad():&#10;                times = []&#10;                rss0 = proc.memory_info().rss&#10;                for _ in range(iters):&#10;                    t0 = time.perf_counter()&#10;                    _ = model(x)&#10;                    torch.cpu.synchronize() if device.type == &quot;cuda&quot; else None&#10;                    t1 = time.perf_counter()&#10;                    times.append((t1 - t0) * 1000.0)  # ms&#10;                    peak_rss = max(peak_rss, proc.memory_info().rss)&#10;                latencies.extend(times)&#10;        pct = _percentiles(latencies)&#10;        mean_ms = float(np.mean(latencies))&#10;        throughput = (bs / (mean_ms / 1000.0))  # img/s for mean latency&#10;        results[bs] = {&#10;            &quot;latency_p50_ms&quot;: pct[&quot;p50&quot;],&#10;            &quot;latency_p90_ms&quot;: pct[&quot;p90&quot;],&#10;            &quot;throughput_img_s&quot;: throughput,&#10;            &quot;peak_ram_mb&quot;: (peak_rss - proc.memory_info().rss) / (1024 * 1024),&#10;        }&#10;    return results&#10;&#10;&#10;def cpu_info() -&gt; Dict[str, str]:&#10;    try:&#10;        import cpuinfo&#10;        info = cpuinfo.get_cpu_info()&#10;        return {&#10;            &quot;brand_raw&quot;: info.get(&quot;brand_raw&quot;, &quot;&quot;),&#10;            &quot;arch&quot;: info.get(&quot;arch&quot;, &quot;&quot;),&#10;            &quot;bits&quot;: str(info.get(&quot;bits&quot;, &quot;&quot;)),&#10;            &quot;count&quot;: str(psutil.cpu_count(logical=True)),&#10;        }&#10;    except Exception:&#10;        return {&quot;brand_raw&quot;: &quot;&quot;, &quot;arch&quot;: &quot;&quot;, &quot;bits&quot;: &quot;&quot;, &quot;count&quot;: str(psutil.cpu_count())}&#10;&#10;" />
              <option name="updatedContent" value="import time&#10;from typing import Dict, List, Tuple, Optional&#10;&#10;import numpy as np&#10;import torch&#10;import psutil&#10;import os&#10;&#10;&#10;def _percentiles(xs: List[float], ps=(50, 90, 95, 99)) -&gt; Dict[str, float]:&#10;    arr = np.array(xs)&#10;    out = {}&#10;    for p in ps:&#10;        out[f&quot;p{p}&quot;] = float(np.percentile(arr, p))&#10;    return out&#10;&#10;&#10;def measure_model_size_mb(state_dict_path: Optional[str] = None, model: Optional[torch.nn.Module] = None) -&gt; float:&#10;    if state_dict_path and os.path.exists(state_dict_path):&#10;        return os.path.getsize(state_dict_path) / (1024 * 1024)&#10;    if model is not None:&#10;        tmp = &quot;_tmp_model_size.pt&quot;&#10;        torch.save(model.state_dict(), tmp)&#10;        size = os.path.getsize(tmp) / (1024 * 1024)&#10;        os.remove(tmp)&#10;        return size&#10;    return float(&quot;nan&quot;)&#10;&#10;&#10;def benchmark_torch(&#10;    model: torch.nn.Module,&#10;    input_shape: Tuple[int, int, int, int] = (1, 3, 224, 224),&#10;    device: torch.device = torch.device(&quot;cpu&quot;),&#10;    batch_sizes: Tuple[int, ...] = (1, 8),&#10;    warmup: int = 50,&#10;    iters: int = 100,&#10;    repeats: int = 3,&#10;    num_threads: Optional[int] = None,&#10;) -&gt; Dict[int, Dict[str, float]]:&#10;    &quot;&quot;&quot;&#10;    Метрики на батч:&#10;      - latency p50/p90 (мс)&#10;      - throughput (img/s)&#10;      - peak RAM (MB) — пик роста RSS за замеры&#10;    &quot;&quot;&quot;&#10;    if num_threads is not None:&#10;        torch.set_num_threads(num_threads)&#10;&#10;    model.eval().to(device)&#10;&#10;    results = {}&#10;    proc = psutil.Process(os.getpid())&#10;&#10;    for bs in batch_sizes:&#10;        x = torch.randn(bs, *input_shape[1:], device=device)&#10;        # прогрев&#10;        with torch.no_grad():&#10;            for _ in range(warmup):&#10;                _ = model(x)&#10;                if device.type == &quot;cuda&quot;:&#10;                    torch.cuda.synchronize()&#10;        latencies = []&#10;        max_peak_delta = 0&#10;        for _ in range(repeats):&#10;            with torch.no_grad():&#10;                times = []&#10;                rss0 = proc.memory_info().rss&#10;                peak_rss = rss0&#10;                for _ in range(iters):&#10;                    t0 = time.perf_counter()&#10;                    _ = model(x)&#10;                    if device.type == &quot;cuda&quot;:&#10;                        torch.cuda.synchronize()&#10;                    t1 = time.perf_counter()&#10;                    times.append((t1 - t0) * 1000.0)  # ms&#10;                    peak_rss = max(peak_rss, proc.memory_info().rss)&#10;                latencies.extend(times)&#10;                max_peak_delta = max(max_peak_delta, peak_rss - rss0)&#10;        pct = _percentiles(latencies)&#10;        mean_ms = float(np.mean(latencies))&#10;        throughput = (bs / (mean_ms / 1000.0))  # img/s&#10;        results[bs] = {&#10;            &quot;latency_p50_ms&quot;: pct[&quot;p50&quot;],&#10;            &quot;latency_p90_ms&quot;: pct[&quot;p90&quot;],&#10;            &quot;throughput_img_s&quot;: throughput,&#10;            &quot;peak_ram_mb&quot;: max_peak_delta / (1024 * 1024),&#10;        }&#10;    return results&#10;&#10;&#10;def cpu_info() -&gt; Dict[str, str]:&#10;    try:&#10;        import cpuinfo&#10;        info = cpuinfo.get_cpu_info()&#10;        return {&#10;            &quot;brand_raw&quot;: info.get(&quot;brand_raw&quot;, &quot;&quot;),&#10;            &quot;arch&quot;: info.get(&quot;arch&quot;, &quot;&quot;),&#10;            &quot;bits&quot;: str(info.get(&quot;bits&quot;, &quot;&quot;)),&#10;            &quot;count&quot;: str(psutil.cpu_count(logical=True)),&#10;        }&#10;    except Exception:&#10;        return {&quot;brand_raw&quot;: &quot;&quot;, &quot;arch&quot;: &quot;&quot;, &quot;bits&quot;: &quot;&quot;, &quot;count&quot;: str(psutil.cpu_count())}" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/utils/data.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/utils/data.py" />
              <option name="updatedContent" value="import numpy as np&#10;from typing import Tuple, List, Optional&#10;import torch&#10;from torch.utils.data import DataLoader, Subset&#10;from torchvision import transforms&#10;import medmnist&#10;from medmnist import INFO&#10;from sklearn.model_selection import StratifiedKFold&#10;&#10;&#10;def get_transforms(img_size: int = 224):&#10;    train_tf = transforms.Compose([&#10;        transforms.Resize((img_size, img_size)),&#10;        # аугментации только для train, простые чтобы честно сравнивать&#10;        transforms.RandomHorizontalFlip(),&#10;        transforms.RandomVerticalFlip(),&#10;        transforms.ToTensor(),&#10;        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] == 1 else x),&#10;    ])&#10;    eval_tf = transforms.Compose([&#10;        transforms.Resize((img_size, img_size)),&#10;        transforms.ToTensor(),&#10;        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] == 1 else x),&#10;    ])&#10;    return train_tf, eval_tf&#10;&#10;&#10;def get_medmnist(dataset_name: str = 'pathmnist', img_size: int = 224, download: bool = True):&#10;    info = INFO[dataset_name]&#10;    DataClass = getattr(medmnist, info['python_class'])&#10;    train_tf, eval_tf = get_transforms(img_size)&#10;    train_ds = DataClass(split='train', transform=train_tf, download=download)&#10;    val_ds = DataClass(split='val', transform=eval_tf, download=download)&#10;    test_ds = DataClass(split='test', transform=eval_tf, download=download)&#10;    num_classes = len(info['label'])&#10;    return train_ds, val_ds, test_ds, num_classes&#10;&#10;&#10;def build_loaders(train_ds, val_ds, test_ds, batch_size: int = 64, num_workers: int = 2):&#10;    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)&#10;    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)&#10;    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)&#10;    return train_loader, val_loader, test_loader&#10;&#10;&#10;def stratified_kfold_splits(dataset, n_splits: int = 5, seed: int = 42) -&gt; List[Tuple[Subset, Subset]]:&#10;    # для PathMNIST dataset[i] -&gt; (img, target) где target может быть one-hot&#10;    y = []&#10;    for i in range(len(dataset)):&#10;        _, t = dataset[i]&#10;        if isinstance(t, torch.Tensor) and t.ndim &gt; 1:&#10;            t = t.argmax(dim=0).item()&#10;        elif isinstance(t, torch.Tensor):&#10;            t = int(t.item())&#10;        y.append(t)&#10;    y = np.array(y)&#10;    idxs = np.arange(len(dataset))&#10;    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)&#10;    splits = []&#10;    for tr_idx, val_idx in skf.split(idxs, y):&#10;        splits.append((Subset(dataset, tr_idx), Subset(dataset, val_idx)))&#10;    return splits&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/utils/kd.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/utils/kd.py" />
              <option name="updatedContent" value="import torch&#10;import torch.nn as nn&#10;import torch.nn.functional as F&#10;&#10;&#10;class DistillationLoss(nn.Module):&#10;    &quot;&quot;&quot;&#10;    KD loss (Hinton et al.):&#10;      L = (1-α) * CE(y_s, y) + α * T^2 * KL(softmax(z_t/T) || softmax(z_s/T))&#10;    where z_s, z_t are student/teacher logits, T is temperature.&#10;    &quot;&quot;&quot;&#10;    def __init__(self, alpha: float = 0.5, temperature: float = 4.0):&#10;        super().__init__()&#10;        self.alpha = alpha&#10;        self.temperature = temperature&#10;        self.ce = nn.CrossEntropyLoss()&#10;        self.kl = nn.KLDivLoss(reduction='batchmean')&#10;&#10;    def forward(self, z_s: torch.Tensor, z_t: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:&#10;        if y.ndim &gt; 1:&#10;            y = torch.argmax(y, dim=1)&#10;        y = y.long()&#10;        ce = self.ce(z_s, y)&#10;        zs = z_s / self.temperature&#10;        zt = z_t / self.temperature&#10;        kd = self.kl(F.log_softmax(zs, dim=1), F.softmax(zt, dim=1)) * (self.temperature ** 2)&#10;        return (1 - self.alpha) * ce + self.alpha * kd&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/utils/metrics.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/utils/metrics.py" />
              <option name="updatedContent" value="from typing import Dict, Tuple, Optional&#10;import numpy as np&#10;import torch&#10;import torch.nn.functional as F&#10;from sklearn.metrics import (&#10;    accuracy_score,&#10;    f1_score,&#10;    precision_recall_fscore_support,&#10;    roc_auc_score,&#10;    confusion_matrix,&#10;)&#10;&#10;&#10;def _to_numpy(t: torch.Tensor) -&gt; np.ndarray:&#10;    return t.detach().cpu().numpy()&#10;&#10;&#10;def collect_logits_labels(model: torch.nn.Module, loader, device: torch.device) -&gt; Tuple[np.ndarray, np.ndarray]:&#10;    model.eval()&#10;    logits_list, labels_list = [], []&#10;    with torch.no_grad():&#10;        for x, y in loader:&#10;            x = x.to(device)&#10;            if isinstance(y, torch.Tensor):&#10;                y = y.to(device)&#10;            out = model(x)&#10;            if isinstance(y, torch.Tensor) and y.ndim &gt; 1:&#10;                y = y.argmax(dim=1)&#10;            logits_list.append(_to_numpy(out))&#10;            labels_list.append(_to_numpy(y.long()))&#10;    logits = np.concatenate(logits_list, axis=0)&#10;    labels = np.concatenate(labels_list, axis=0)&#10;    return logits, labels&#10;&#10;&#10;def compute_metrics_from_logits(logits: np.ndarray, labels: np.ndarray, num_classes: Optional[int] = None) -&gt; Dict[str, float]:&#10;    preds = logits.argmax(axis=1)&#10;    acc = accuracy_score(labels, preds)&#10;    macro_f1 = f1_score(labels, preds, average='macro')&#10;    precision, recall, f1_per_class, _ = precision_recall_fscore_support(labels, preds, average=None)&#10;    # ROC-AUC макро; нужен one-vs-rest, если все классы присутствуют&#10;    roc_auc = np.nan&#10;    try:&#10;        if num_classes is None:&#10;            num_classes = int(np.max(labels) + 1)&#10;        y_true_oh = np.eye(num_classes)[labels]&#10;        probs = softmax_np(logits)&#10;        roc_auc = roc_auc_score(y_true_oh, probs, average='macro', multi_class='ovr')&#10;    except Exception:&#10;        pass&#10;    cm = confusion_matrix(labels, preds)&#10;    metrics = {&#10;        'accuracy': float(acc),&#10;        'macro_f1': float(macro_f1),&#10;        'roc_auc_macro': float(roc_auc) if not np.isnan(roc_auc) else float('nan'),&#10;    }&#10;    # добавим средние precision/recall&#10;    metrics['macro_precision'] = float(np.nanmean(precision))&#10;    metrics['macro_recall'] = float(np.nanmean(recall))&#10;    # вернём и вспомогательные&#10;    metrics['_precision_per_class'] = precision.tolist()&#10;    metrics['_recall_per_class'] = recall.tolist()&#10;    metrics['_f1_per_class'] = f1_per_class.tolist()&#10;    metrics['_confusion_matrix'] = cm.astype(int).tolist()&#10;    return metrics&#10;&#10;&#10;def softmax_np(logits: np.ndarray) -&gt; np.ndarray:&#10;    z = logits - logits.max(axis=1, keepdims=True)&#10;    exp = np.exp(z)&#10;    return exp / exp.sum(axis=1, keepdims=True)&#10;&#10;&#10;def evaluate(model: torch.nn.Module, loader, device: torch.device, num_classes: Optional[int] = None) -&gt; Dict[str, float]:&#10;    logits, labels = collect_logits_labels(model, loader, device)&#10;    return compute_metrics_from_logits(logits, labels, num_classes)&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/utils/quant.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/utils/quant.py" />
              <option name="originalContent" value="    from typing import Optional, Iterable&#10;import torch&#10;import torch.nn as nn&#10;&#10;&#10;def dynamic_int8(model: nn.Module, modules: Optional[Iterable[type]] = None) -&gt; nn.Module:&#10;    &quot;&quot;&quot;&#10;    PTQ dynamic: quantize Linear (and optionally Conv2d) to int8 weights, activations dynamically quantized.&#10;    Works well for ViT (Linear-heavy).&#10;    &quot;&quot;&quot;&#10;    if modules is None:&#10;        modules = {nn.Linear}&#10;    qmodel = torch.quantization.quantize_dynamic(model, modules, dtype=torch.qint8)&#10;    return qmodel&#10;&#10;&#10;def static_ptq_fx(model: nn.Module, calib_loader, backend: str = 'fbgemm', calib_batches: int = 100) -&gt; nn.Module:&#10;    &quot;&quot;&quot;&#10;    PTQ static via FX graph mode. Recommended for CNN baselines (ResNet/MobileNet/ConvNeXt parts).&#10;    For ViT models, prefer dynamic_int8.&#10;    &quot;&quot;&quot;&#10;    torch.backends.quantized.engine = backend&#10;    model.eval()&#10;    example_inputs = next(iter(calib_loader))[0]&#10;    example_inputs = example_inputs[:1]&#10;    try:&#10;        from torch.ao.quantization import get_default_qconfig&#10;        from torch.ao.quantization.fx import prepare_fx, convert_fx&#10;    except Exception as e:&#10;        raise RuntimeError(f&quot;FX quantization not available: {e}&quot;)&#10;&#10;    qconfig = get_default_qconfig(backend)&#10;    qconfig_dict = {&quot;&quot;: qconfig}&#10;    prepared = prepare_fx(model, qconfig_dict, example_inputs=example_inputs)&#10;    # calibration&#10;    prepared.eval()&#10;    with torch.no_grad():&#10;        for i, (x, _) in enumerate(calib_loader):&#10;            _ = prepared(x)&#10;            if i + 1 &gt;= calib_batches:&#10;                break&#10;    qmodel = convert_fx(prepared)&#10;    return qmodel&#10;&#10;&#10;def qat_lite_fx(model: nn.Module, train_loader, val_loader=None, epochs: int = 5, lr: float = 5e-5,&#10;                 backend: str = 'fbgemm', device: torch.device = torch.device('cpu'), teacher: Optional[nn.Module] = None,&#10;                 alpha: float = 0.5, temperature: float = 4.0) -&gt; nn.Module:&#10;    &quot;&quot;&quot;&#10;    QAT-lite: quick fake-quant training (3–10 epochs) with small LR. Optionally with KD from teacher.&#10;    Note: robust for CNN baselines; ViT support depends on FX coverage.&#10;    &quot;&quot;&quot;&#10;    from torch.ao.quantization import get_default_qat_qconfig&#10;    from torch.ao.quantization.fx import prepare_qat_fx, convert_fx&#10;    import torch.optim as optim&#10;    import torch.nn.functional as F&#10;&#10;    torch.backends.quantized.engine = backend&#10;    model.train().to(device)&#10;    example_inputs = next(iter(train_loader))[0][:1].to(device)&#10;    qconfig = get_default_qat_qconfig(backend)&#10;    qconfig_dict = {&quot;&quot;: qconfig}&#10;&#10;    qat_model = prepare_qat_fx(model, qconfig_dict, example_inputs=example_inputs)&#10;    optimizer = optim.AdamW(qat_model.parameters(), lr=lr)&#10;&#10;    def kd_loss(z_s, y, z_t=None):&#10;        ce = torch.nn.functional.cross_entropy(z_s, y)&#10;        if z_t is None:&#10;            return ce&#10;        zs = z_s / temperature&#10;        zt = z_t / temperature&#10;        kl = torch.nn.functional.kl_div(torch.log_softmax(zs, dim=1), torch.softmax(zt, dim=1), reduction='batchmean')&#10;        return (1 - alpha) * ce + (alpha) * (temperature ** 2) * kl&#10;&#10;    for epoch in range(epochs):&#10;        for x, y in train_loader:&#10;            x = x.to(device)&#10;            if y.ndim &gt; 1:&#10;                y = y.argmax(dim=1)&#10;            y = y.to(device).long()&#10;            optimizer.zero_grad()&#10;            if teacher is not None:&#10;                with torch.no_grad():&#10;                    z_t = teacher(x)&#10;            else:&#10;                z_t = None&#10;            z_s = qat_model(x)&#10;            loss = kd_loss(z_s, y, z_t)&#10;            loss.backward()&#10;            optimizer.step()&#10;    qat_model.eval()&#10;    qmodel = convert_fx(qat_model)&#10;    return qmodel&#10;&#10;" />
              <option name="updatedContent" value="from typing import Optional, Iterable&#10;import torch&#10;import torch.nn as nn&#10;&#10;&#10;def dynamic_int8(model: nn.Module, modules: Optional[Iterable[type]] = None) -&gt; nn.Module:&#10;    &quot;&quot;&quot;&#10;    PTQ dynamic: квантование весов (int8) и динамическая квантизация активаций.&#10;    Для ViT используем Linear-слои.&#10;    &quot;&quot;&quot;&#10;    if modules is None:&#10;        modules = {nn.Linear}&#10;    qmodel = torch.quantization.quantize_dynamic(model, modules, dtype=torch.qint8)&#10;    return qmodel&#10;&#10;&#10;def static_ptq_fx(model: nn.Module, calib_loader, backend: str = 'fbgemm', calib_batches: int = 100) -&gt; nn.Module:&#10;    &quot;&quot;&quot;&#10;    PTQ static через FX graph mode. Рекомендовано для CNN-бейзлайнов.&#10;    &quot;&quot;&quot;&#10;    torch.backends.quantized.engine = backend&#10;    model.eval()&#10;    example_inputs = next(iter(calib_loader))[0]&#10;    example_inputs = example_inputs[:1]&#10;    try:&#10;        from torch.ao.quantization import get_default_qconfig&#10;        from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx&#10;    except Exception as e:&#10;        raise RuntimeError(f&quot;FX quantization not available: {e}&quot;)&#10;&#10;    qconfig = get_default_qconfig(backend)&#10;    qconfig_dict = {&quot;&quot;: qconfig}&#10;    prepared = prepare_fx(model, qconfig_dict, example_inputs=example_inputs)&#10;    # calibration&#10;    prepared.eval()&#10;    with torch.no_grad():&#10;        for i, (x, _) in enumerate(calib_loader):&#10;            _ = prepared(x)&#10;            if i + 1 &gt;= calib_batches:&#10;                break&#10;    qmodel = convert_fx(prepared)&#10;    return qmodel&#10;&#10;&#10;def qat_lite_fx(model: nn.Module, train_loader, val_loader=None, epochs: int = 5, lr: float = 5e-5,&#10;                 backend: str = 'fbgemm', device: torch.device = torch.device('cpu'), teacher: Optional[nn.Module] = None,&#10;                 alpha: float = 0.5, temperature: float = 4.0) -&gt; nn.Module:&#10;    &quot;&quot;&quot;&#10;    QAT-lite: короткий fake-quant тренинг (3–10 эпох) с малым LR. Опционально KD.&#10;    Примечание: лучше поддержан для CNN; для ViT может не работать.&#10;    &quot;&quot;&quot;&#10;    try:&#10;        from torch.ao.quantization import get_default_qat_qconfig&#10;        from torch.ao.quantization.quantize_fx import prepare_qat_fx, convert_fx&#10;    except Exception as e:&#10;        raise RuntimeError(f&quot;FX QAT not available: {e}&quot;)&#10;    import torch.optim as optim&#10;&#10;    torch.backends.quantized.engine = backend&#10;    model.to(device)&#10;    example_inputs = next(iter(train_loader))[0][:1].to(device)&#10;    qconfig = get_default_qat_qconfig(backend)&#10;    qconfig_dict = {&quot;&quot;: qconfig}&#10;&#10;    qat_model = prepare_qat_fx(model, qconfig_dict, example_inputs=example_inputs)&#10;    optimizer = optim.AdamW(qat_model.parameters(), lr=lr)&#10;&#10;    def kd_loss(z_s, y, z_t=None):&#10;        ce = torch.nn.functional.cross_entropy(z_s, y)&#10;        if z_t is None:&#10;            return ce&#10;        zs = z_s / temperature&#10;        zt = z_t / temperature&#10;        kl = torch.nn.functional.kl_div(torch.log_softmax(zs, dim=1), torch.softmax(zt, dim=1), reduction='batchmean')&#10;        return (1 - alpha) * ce + (alpha) * (temperature ** 2) * kl&#10;&#10;    qat_model.train()&#10;    for _ in range(epochs):&#10;        for x, y in train_loader:&#10;            x = x.to(device)&#10;            if y.ndim &gt; 1:&#10;                y = y.argmax(dim=1)&#10;            y = y.to(device).long()&#10;            optimizer.zero_grad()&#10;            if teacher is not None:&#10;                with torch.no_grad():&#10;                    z_t = teacher(x)&#10;            else:&#10;                z_t = None&#10;            z_s = qat_model(x)&#10;            loss = kd_loss(z_s, y, z_t)&#10;            loss.backward()&#10;            optimizer.step()&#10;    qat_model.eval()&#10;    qmodel = convert_fx(qat_model)&#10;    return qmodel" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>